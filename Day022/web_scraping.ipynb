{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises: Day 22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been successfully scraped and stored as \"scraped_data.json\".\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "\n",
    "url = 'http://www.bu.edu/president/boston-university-facts-stats/'\n",
    "response = requests.get(url)\n",
    "# To know if the results is successful\n",
    "if response.status_code == 200:\n",
    "    bueat_soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    # Find the data you want to scrape\n",
    "    paragraphs = bueat_soup.find_all('p')\n",
    "    scarpe_data_list = []\n",
    "    # Iterate the list\n",
    "    for paragraph in paragraphs:\n",
    "        scarpe_data_list.append(paragraph.get_text())\n",
    "    # Create a dictionary to store the data\n",
    "    data_dict = {'paragraphs': scarpe_data_list}\n",
    "    # to JSON format\n",
    "    json_data = json.dumps(data_dict, indent=4)\n",
    "     # Save the JSON data to a file\n",
    "    with open('scraped_data.json', 'w') as json_file:\n",
    "        json_file.write(json_data)\n",
    "\n",
    "    print('Data has been successfully scraped and stored as \"scraped_data.json\".')\n",
    "\n",
    "else:\n",
    "    print(f'Failed to retrieve data. Status code: {response.status_code}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-2.0.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/halima/miniconda3/envs/ArewaDS/lib/python3.8/site-packages (from pandas) (2.8.2)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Downloading pytz-2023.3.post1-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.1 (from pandas)\n",
      "  Downloading tzdata-2023.4-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: numpy>=1.20.3 in /home/halima/miniconda3/envs/ArewaDS/lib/python3.8/site-packages (from pandas) (1.24.3)\n",
      "Requirement already satisfied: six>=1.5 in /home/halima/miniconda3/envs/ArewaDS/lib/python3.8/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Downloading pandas-2.0.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m704.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pytz-2023.3.post1-py2.py3-none-any.whl (502 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m502.5/502.5 kB\u001b[0m \u001b[31m742.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading tzdata-2023.4-py2.py3-none-any.whl (346 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m346.6/346.6 kB\u001b[0m \u001b[31m235.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pytz, tzdata, pandas\n",
      "Successfully installed pandas-2.0.3 pytz-2023.3.post1 tzdata-2023.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to retrieve data. Status code: 404\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "url = 'https://archive.ics.uci.edu/ml/datasets.php'\n",
    "get_response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if get_response.status_code == 200:\n",
    "    soup = BeautifulSoup(get_response.content, 'html.parser')\n",
    "\n",
    "    # To Find the table on the page using BeautifulSoup methods\n",
    "    page_table = soup.find('table')\n",
    "\n",
    "    # Use pandas to read the HTML table into a DataFrame\n",
    "    df = pd.read_html(str(page_table))[0]\n",
    "\n",
    "    # Convert the DataFrame to a JSON object\n",
    "    json_data = df.to_json(orient='records', indent=4)\n",
    "\n",
    "    # Save the JSON data to a file\n",
    "    with open('table_data.json', 'w') as json_file:\n",
    "        json_file.write(json_data)\n",
    "        print('Table data has been successfully extracted and stored as \"table_data.json\".')\n",
    "\n",
    "else:\n",
    "    print(f'Failed to retrieve data. Status code: {get_response.status_code}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Presidents data has been successfully scraped and stored as \"presidents_data.json\".\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "\n",
    "url = 'https://en.wikipedia.org/wiki/List_of_presidents_of_the_United_States'\n",
    "response = requests.get(url)\n",
    "if response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    # Find the table \n",
    "    tbl = soup.find('table', {'class': 'wikitable'})\n",
    "    # Extract the table rows\n",
    "    rows = tbl.find_all('tr')\n",
    "    # Create a list to store the data\n",
    "    lst = []\n",
    "    # Iterate through the rows and extract the data\n",
    "    for row in rows[1:]: \n",
    "        columns = row.find_all(['th', 'td'])\n",
    "        president_datas = [column.get_text(strip=True) for column in columns]\n",
    "        lst.append(president_datas)\n",
    "\n",
    "    # Create a list of dictionaries representing each president's data\n",
    "    presidents_data = []\n",
    "    for president_data in lst:\n",
    "        president_dict = {\n",
    "            \"Number\": president_data[0],\n",
    "            \"President\": president_data[1],\n",
    "            \"Took office\": president_data[2],\n",
    "            \"Left office\": president_data[3],\n",
    "            \"Party\": president_data[4],\n",
    "            \"Home state\": president_data[5]\n",
    "        }\n",
    "        presidents_data.append(president_dict)\n",
    "\n",
    "    # Convert the list of dictionaries to JSON format\n",
    "    json_data = json.dumps(presidents_data, indent=4)\n",
    "\n",
    "    # Save the JSON data to a file\n",
    "    with open('presidents_data.json', 'w') as json_file:\n",
    "        json_file.write(json_data)\n",
    "\n",
    "    print('Presidents data has been successfully scraped and stored as \"presidents_data.json\".')\n",
    "\n",
    "else:\n",
    "    print(f'Failed to retrieve data. Status code: {response.status_code}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ArewaDS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
